{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163e46f4-9040-4e1b-bdd7-3689c3e69e7f",
   "metadata": {},
   "source": [
    "# Text Representations\n",
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc870720-d44f-42f0-b602-761001bb04e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Document-Term Matrix:\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create sample documents\n",
    "documents = [\"This is the first document.\",\n",
    "              \"This document is the second document.\",\n",
    "              \"And this is the third one.\"]\n",
    "\n",
    "# Create the Bag-of-Words model\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the feature names and the document-term matrix\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "print(\"Document-Term Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334ee20-51e9-4dda-b47c-305cfa8f18df",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6824eea9-d46a-4c5d-9464-1d795c11056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['and' 'and this' 'and this is' 'document' 'document is' 'document is the'\n",
      " 'first' 'first document' 'is' 'is the' 'is the first' 'is the second'\n",
      " 'is the third' 'one' 'second' 'second document' 'the' 'the first'\n",
      " 'the first document' 'the second' 'the second document' 'the third'\n",
      " 'the third one' 'third' 'third one' 'this' 'this document'\n",
      " 'this document is' 'this is' 'this is the']\n",
      "Document-Term Matrix:\n",
      " [[0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1]\n",
      " [0 0 0 2 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0]\n",
      " [1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create sample documents\n",
    "documents = [\"This is the first document.\",\n",
    "              \"This document is the second document.\",\n",
    "              \"And this is the third one.\"]\n",
    "\n",
    "# Create the Bag-of-Words model with unigrams, bigrams, and trigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the feature names and the document-term matrix\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "print(\"Document-Term Matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b211349e-1ce9-4984-bef7-0c8ff449b3b6",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "345cdd85-41e5-4088-af5c-f57e1b9fbbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.46941728 0.61722732 0.3645444  0.         0.\n",
      "  0.3645444  0.         0.3645444 ]\n",
      " [0.         0.7284449  0.         0.28285122 0.         0.47890875\n",
      "  0.28285122 0.         0.28285122]\n",
      " [0.49711994 0.         0.         0.29360705 0.49711994 0.\n",
      "  0.29360705 0.49711994 0.29360705]]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create sample documents\n",
    "documents = [\"This is the first document.\",\n",
    "              \"This document is the second document.\",\n",
    "              \"And this is the third one.\"]\n",
    "\n",
    "# Create the TF-IDF model\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the feature names and the TF-IDF matrix\n",
    "print(\"Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefb33a-ddc3-40e3-9bd0-4cede12bbd0b",
   "metadata": {},
   "source": [
    "# Word Embedding : Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d01f316d-f481-4452-bdcd-0b04d87eaefe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Install gensim library if not already installed\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# !pip3 install \"gensim==3.8.2\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Install gensim library if not already installed\n",
    "# !pip3 install \"gensim==3.8.2\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create sample documents\n",
    "documents = [\"This is the first document.\",\n",
    "              \"This document is the second document.\",\n",
    "              \"And this is the third one.\"]\n",
    "\n",
    "# Tokenize the documents\n",
    "tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Example: Get vector representation for the word 'document'\n",
    "vector_representation = model.wv['document']\n",
    "print(\"Vector Representation for 'document':\", vector_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247aa2d3-d03b-45cc-a8e6-90a5fb01672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Subject: Remote Work Opportunity Inquiry from Ireland for Data Analyst Role\n",
    "\n",
    "Hello,\n",
    "\n",
    "I hope this email finds you well. My name is Merve, and I am writing with keen interest in the Data Analyst position at Ada Health.\n",
    "\n",
    "With a strong background in data analysis, including expertise in SQL and Tableau, I am excited about the opportunity to contribute to Ada's vision. I am currently based in Ireland and would like to inquire about the possibility of working remotely for this role.\n",
    "\n",
    "I am impressed by Ada's commitment to global health and innovation, and I believe my skills and experience would be a valuable addition to your team. I am eager to discuss how I can contribute to Ada Health in this capacity.\n",
    "\n",
    "Thank you for considering my request. I look forward to the opportunity to discuss this further.\n",
    "\n",
    "Best regards,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
